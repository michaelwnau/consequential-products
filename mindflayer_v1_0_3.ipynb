{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyO70AheVT53iw9lczUUvgSz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michaelwnau/consequential-products/blob/main/mindflayer_v1_0_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UWAON7epe6p"
      },
      "outputs": [],
      "source": [
        "# Setup and Imports\n",
        "!pip install pdfplumber transformers torch Pillow matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "import os\n",
        "import io\n",
        "from PIL import Image\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import numpy as np\n",
        "import textwrap\n",
        "from google.colab import drive, files"
      ],
      "metadata": {
        "id": "Wo6u-ulUpjwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "gY0FkczopmuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the path to your PDF file in Google Drive or other repository\n",
        "pdf_file_path = '/content/drive/MyDrive/DTRA243-003-RESEARCH/s41467-024-45563-x.pdf'"
      ],
      "metadata": {
        "id": "rt2r27_FppXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Extract Text and Images from PDF\n",
        "def extract_text_and_images(pdf_path):\n",
        "    text = \"\"\n",
        "    images = []\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            text += page.extract_text() or \"\"\n",
        "            for image in page.images:\n",
        "                images.append(image)\n",
        "    return text, images\n",
        "\n",
        "text, images = extract_text_and_images(pdf_file_path)\n",
        "\n",
        "print(\"Extracted Text Snippet:\\n\")\n",
        "print(text[:500])"
      ],
      "metadata": {
        "id": "zgRX_hbeps1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Save Extracted Images\n",
        "os.makedirs('extracted_images', exist_ok=True)\n",
        "image_paths = []\n",
        "\n",
        "for i, img in enumerate(images):\n",
        "    try:\n",
        "        image_path = f'extracted_images/image_{i}.png'\n",
        "        pil_image = Image.open(io.BytesIO(img['stream'].get_data()))\n",
        "\n",
        "        # Convert to RGB if the image has an alpha channel\n",
        "        if pil_image.mode in ('RGBA', 'LA') or (pil_image.mode == 'P' and 'transparency' in pil_image.info):\n",
        "            pil_image = pil_image.convert('RGB')\n",
        "\n",
        "        pil_image.save(image_path, 'PNG')\n",
        "        image_paths.append(image_path)\n",
        "        print(f\"Saved image {i}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image {i}: {str(e)}\")\n",
        "\n",
        "print(f\"Extracted and saved {len(image_paths)} images\")"
      ],
      "metadata": {
        "id": "8eHQhs_Np1Nl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Interpret Images Using a Multimodal Model (BLIP)\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "captions = {}\n",
        "\n",
        "for image_path in image_paths:\n",
        "    try:\n",
        "        raw_image = Image.open(image_path).convert('RGB')\n",
        "        inputs = processor(raw_image, return_tensors=\"pt\").to(torch.device('cpu'))\n",
        "        out = model.generate(**inputs)\n",
        "        caption = processor.decode(out[0], skip_special_tokens=True)\n",
        "        captions[image_path] = caption\n",
        "        print(f\"Generated caption for {image_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {image_path}: {str(e)}\")\n",
        "        captions[image_path] = \"Caption generation failed\""
      ],
      "metadata": {
        "id": "Z4Rs8Wjxp4p1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# 3. Interpret Images Using a Multimodal Model (BLIP)\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "import io\n",
        "\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "captions = {}\n",
        "\n",
        "for image_path in image_paths:\n",
        "    try:\n",
        "        raw_image = Image.open(image_path).convert('RGB')\n",
        "    except UnidentifiedImageError:\n",
        "        # Attempt to open with io.BytesIO if Image.open fails\n",
        "        with open(image_path, 'rb') as f:\n",
        "            image_data = f.read()\n",
        "        try:\n",
        "            raw_image = Image.open(io.BytesIO(image_data)).convert('RGB')\n",
        "        except UnidentifiedImageError:\n",
        "            print(f\"Skipping {image_path}: Could not identify image format.\")\n",
        "            continue\n",
        "\n",
        "    inputs = processor(raw_image, return_tensors=\"pt\").to(torch.device('cpu'))\n",
        "    out = model.generate(**inputs)\n",
        "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
        "    captions[image_path] = caption"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "g791WhZRqx_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Display Results\n",
        "fig = plt.figure(figsize=(15, len(image_paths) * 3))\n",
        "gs = gridspec.GridSpec(len(image_paths), 2, width_ratios=[1, 1], hspace=0.5)\n",
        "\n",
        "for idx, image_path in enumerate(image_paths):\n",
        "    # Image subplot\n",
        "    ax_img = fig.add_subplot(gs[idx, 0])\n",
        "    img = Image.open(image_path)\n",
        "    ax_img.imshow(img)\n",
        "    ax_img.axis('off')\n",
        "\n",
        "    # Caption subplot\n",
        "    ax_text = fig.add_subplot(gs[idx, 1])\n",
        "    ax_text.axis('off')\n",
        "\n",
        "    caption = captions.get(image_path, 'No caption available')\n",
        "    wrapped_caption = textwrap.fill(f\"Caption: {caption}\", width=60)\n",
        "\n",
        "    ax_text.text(0.5, 0.5, wrapped_caption,\n",
        "                 ha='center', va='center', fontsize=8,\n",
        "                 bbox=dict(facecolor='white', alpha=0.5, edgecolor='none'),\n",
        "                 wrap=True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save the figure as a PDF\n",
        "pdf_filename = 'results_matplotlib.pdf'\n",
        "fig.savefig(pdf_filename, bbox_inches='tight')\n",
        "\n",
        "# Download the PDF file\n",
        "files.download(pdf_filename)"
      ],
      "metadata": {
        "id": "b0SDUNxwp_m_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Advanced Interpretation (Optional): Summarize the Extracted Text\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "import warnings\n",
        "\n",
        "# Suppress the FutureWarning\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "model_name = \"facebook/bart-large-cnn\"\n",
        "summarizer = pipeline(\"summarization\", model=model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove any non-printable characters\n",
        "    text = ''.join(char for char in text if char.isprintable())\n",
        "    # Remove excessive whitespace\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "\n",
        "def chunk_text(text, max_chunk_size):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_size = 0\n",
        "    for word in words:\n",
        "        if current_size + len(word) + 1 > max_chunk_size:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "            current_chunk = [word]\n",
        "            current_size = len(word)\n",
        "        else:\n",
        "            current_chunk.append(word)\n",
        "            current_size += len(word) + 1\n",
        "    if current_chunk:\n",
        "        chunks.append(' '.join(current_chunk))\n",
        "    return chunks\n",
        "\n",
        "cleaned_text = clean_text(text)\n",
        "\n",
        "if not cleaned_text:\n",
        "    print(\"The extracted text is empty or contains only non-printable characters.\")\n",
        "else:\n",
        "    # Tokenize the text to get the number of tokens\n",
        "    inputs = tokenizer(cleaned_text, return_tensors='pt', truncation=False)\n",
        "    input_ids = inputs['input_ids']\n",
        "    num_tokens = input_ids.shape[1]\n",
        "    print(f\"Number of tokens in extracted text: {num_tokens}\")\n",
        "\n",
        "    # Model's maximum input length\n",
        "    max_model_length = 1024  # BART models typically have a max length of 1024 tokens\n",
        "    print(f\"Model's maximum input length: {max_model_length} tokens\")\n",
        "\n",
        "    # Chunk the text\n",
        "    text_chunks = chunk_text(cleaned_text, max_model_length)\n",
        "    print(f\"Number of chunks: {len(text_chunks)}\")\n",
        "\n",
        "    # Summarize each chunk\n",
        "    summaries = []\n",
        "    for i, chunk in enumerate(text_chunks):\n",
        "        if chunk.strip():  # Ensure the chunk is not empty\n",
        "            try:\n",
        "                summary = summarizer(chunk, max_length=150, min_length=40, do_sample=False, clean_up_tokenization_spaces=True)\n",
        "                summaries.append(summary[0]['summary_text'])\n",
        "                print(f\"Summarized chunk {i+1}/{len(text_chunks)}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error summarizing chunk {i+1}: {str(e)}\")\n",
        "                print(\"Chunk content:\", chunk[:100] + \"...\" if len(chunk) > 100 else chunk)\n",
        "\n",
        "    # Combine summaries\n",
        "    if summaries:\n",
        "        full_summary = ' '.join(summaries)\n",
        "        print(\"\\nSummary of the Article:\\n\")\n",
        "        print(full_summary)\n",
        "    else:\n",
        "        print(\"No summaries were generated. The text might be too short or couldn't be processed.\")\n"
      ],
      "metadata": {
        "id": "riTEg5HpqFtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine summaries\n",
        "full_summary = ' '.join(summaries)\n",
        "print(\"Summary of the Article:\\n\")\n",
        "print(full_summary)"
      ],
      "metadata": {
        "id": "jKo1-6KBqZwp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}