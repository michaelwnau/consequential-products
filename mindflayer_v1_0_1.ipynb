{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1IIBMBDfHqy0Ey64k29bALy6BlDiUpaB2",
      "authorship_tag": "ABX9TyNQv8Lua/M8CjNmfYchcDmi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michaelwnau/consequential-products/blob/main/mindflayer_v1_0_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Mindflayer: Analyzing Scientific Articles with OCR and Multimodal Models**\n",
        "\n",
        "###This Colab Notebook demonstrates how to:\n",
        "\n",
        "- Ingest and extract text from a PDF scientific article.\n",
        "- Extract images, figures, and tables from the PDF.\n",
        "- Apply OCR to images to extract embedded text.\n",
        "- Use multimodal models to generate captions for images.\n",
        "- Display and interpret the extracted content."
      ],
      "metadata": {
        "id": "8qQ02CjmWsS4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MlfwIwaDUxLp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "##**Problem Statement**\n",
        "\n",
        "\n",
        "> \"OBJECTIVE:\n",
        ">The Defense Threat Reduction Agency (DTRA) seeks to develop AI/ML models and pipelines capable of identifying, extracting, and processing elements from scanned technical and scientific documents. This project aims to automate the extraction of tables, plots, photos, and other elements embedded within structured and unstructured text, ensuring high fidelity and accuracy in a production environment.\"\n",
        "\n",
        "Source: \"DTRA243-003 AI/ML Data Extraction from Scientific Documents.\" See the [full problem statement](https://www.dodsbirsttr.mil/submissions/api/public/download/solicitationDocuments?solicitation=DOD_SBIR_2024_P1_C3&documentType=INSTRUCTIONS&component=DTRA) here.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MijXQgrT_dP2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Table of Contents**\n",
        "1. Setup\n",
        "2. Ingest the article\n",
        "3. Extract the images from the PDF\n",
        "4. Apply OCR to Extract Text from Images\n",
        "5. Interpret Images Using Multimodal Models\n",
        "6. Extract and Interpret Tables\n",
        "7. Display Results\n",
        "8. Advanced Interpretation (Optional)"
      ],
      "metadata": {
        "id": "mzx2p8RTX0s3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Installing Tesseract OCR**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Tesseract OCR is required for text extraction from images.\n",
        "\n",
        "Download and install it in your local environment using documentation found here: [Tesseract repo](https://github.com/tesseract-ocr/tesseract).\n",
        "\n",
        "Install Tesseract OCR\n",
        "\n",
        "`!sudo apt-get install tesseract-ocr`\n",
        "\n",
        "Set the Tesseract command path (Colab usually has it installed in /usr/bin/tesseract)\n",
        "\n",
        "`pytesseract.pytesseract.tesseract_cmd = r'/usr/bin/tesseract'`\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FGmKEDo-U2I8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Mount Google Drive**\n",
        "For this notebook, we will use Google Drive as our document storage repository. Run this cell and follow the prompts to connect the drive. If you have a document collection already prepared, select it or use the sample document attached here for demostration."
      ],
      "metadata": {
        "id": "ZKMIiMhliyli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "jH2iWn1firs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**1. Setup**\n",
        "First, let's install the necessary libraries by running the cell."
      ],
      "metadata": {
        "id": "GHJXMeccZYvj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "fjDWl44zWqQ1"
      },
      "outputs": [],
      "source": [
        "# Install libraries\n",
        "!pip install PyPDF2\n",
        "!pip install PyMuPDF\n",
        "!pip install pytesseract\n",
        "!pip install Pillow\n",
        "!pip install transformers\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the libraries\n",
        "import PyPDF2\n",
        "import fitz  # PyMuPDF\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "import torch\n",
        "import os\n",
        "from IPython.display import Image as IPythonImage, display\n",
        "import pandas as pd\n",
        "import io"
      ],
      "metadata": {
        "id": "p1Qa5pCDZUzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**2. Ingest the article**\n",
        "With your Google Drive mounted in the setup steps, you will now set the file path to the article you want to ingest. This demonstration targets a single article but with slight modification it is possible to batch ingest files."
      ],
      "metadata": {
        "id": "5e_S3X-ad5WK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace this path with the path to your PDF file in Google Drive\n",
        "pdf_file_path = '/content/drive/MyDrive/technical-manuals-training-data/AEROSPACE-EQUIPMENT-MAINTENANCE-INSPECTION-to-00-20-1.pdf'"
      ],
      "metadata": {
        "id": "aurU_gCk_BeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Extract Text from the PDF"
      ],
      "metadata": {
        "id": "tSDSaQ9RaBYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Open the PDF file from Google Drive\n",
        "pdf_file = open(pdf_file_path, 'rb')\n",
        "pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "\n",
        "# Extract text from each page\n",
        "text = ''\n",
        "for page in pdf_reader.pages:\n",
        "    page_text = page.extract_text()\n",
        "    if page_text:\n",
        "        text += page_text\n",
        "\n",
        "pdf_file.close()\n",
        "\n",
        "# Display a snippet of the extracted text\n",
        "print(\"Extracted Text Snippet:\\n\")\n",
        "print(text[:500])\n"
      ],
      "metadata": {
        "id": "XgEFj_Poj8pB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**3. Extract Images from the PDF**\n",
        "\n"
      ],
      "metadata": {
        "id": "ZBSyxs_Gj-by"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Open the PDF file with PyMuPDF from Google Drive\n",
        "doc = fitz.open(pdf_file_path)\n",
        "\n",
        "# Create a directory to store images\n",
        "os.makedirs('extracted_images', exist_ok=True)\n",
        "\n",
        "# List to store image file paths\n",
        "image_paths = []\n",
        "\n",
        "# Iterate through each page to extract images\n",
        "for page_index in range(len(doc)):\n",
        "    page = doc[page_index]\n",
        "    image_list = page.get_images(full=True)\n",
        "\n",
        "    print(f\"Found {len(image_list)} images on page {page_index+1}\")\n",
        "\n",
        "    for img_index, img in enumerate(image_list, start=1):\n",
        "        xref = img[0]\n",
        "        base_image = doc.extract_image(xref)\n",
        "        image_bytes = base_image[\"image\"]\n",
        "        image_ext = base_image[\"ext\"]\n",
        "        image_name = f\"image_page{page_index+1}_{img_index}.{image_ext}\"\n",
        "        image_path = os.path.join('extracted_images', image_name)\n",
        "\n",
        "        # Save the image\n",
        "        with open(image_path, \"wb\") as image_file:\n",
        "            image_file.write(image_bytes)\n",
        "            image_paths.append(image_path)\n"
      ],
      "metadata": {
        "id": "iiNSwZXRkEpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**4. Apply OCR to Extract Text from Images**"
      ],
      "metadata": {
        "id": "5t67VcxKg5OO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary to store OCR results\n",
        "ocr_results = {}\n",
        "\n",
        "for image_path in image_paths:\n",
        "    # Open the image file\n",
        "    img = Image.open(image_path)\n",
        "\n",
        "    # Perform OCR\n",
        "    text = pytesseract.image_to_string(img)\n",
        "\n",
        "    # Store the result\n",
        "    ocr_results[image_path] = text\n"
      ],
      "metadata": {
        "id": "pIYjI9dTkRLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**5. Interpret Images Using A Multimodal Model**\n",
        "\n",
        "We are using BLIP for image interpretation and captioning. BLIP is an open-source, pre-trained model from Salesforce ([model card](https://huggingface.co/Salesforce/blip-image-captioning-base))."
      ],
      "metadata": {
        "id": "IbC79NR5hD-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the processor and model\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
      ],
      "metadata": {
        "id": "csXZCeJhkd96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Generate Captions for Images\n",
        "BLIP generates terse responses for efficiency but this can be customized with prompts to generate longer captions including adding alt-text for accessibility."
      ],
      "metadata": {
        "id": "Eg9XfP9XiRRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary to store captions\n",
        "captions = {}\n",
        "\n",
        "for image_path in image_paths:\n",
        "    raw_image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "    # Prepare the image for the model\n",
        "    inputs = processor(raw_image, return_tensors=\"pt\").to(torch.device('cpu'))\n",
        "\n",
        "    # Generate caption\n",
        "    out = model.generate(**inputs)\n",
        "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "    # Store the caption\n",
        "    captions[image_path] = caption\n"
      ],
      "metadata": {
        "id": "tJQ7cZruksAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**6. Extract and Interpret Tables**\n",
        "If tables are images, we can attempt to parse them from the OCR text."
      ],
      "metadata": {
        "id": "2wu-pDCdi_cC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for image_path, ocr_text in ocr_results.items():\n",
        "    if 'table' in image_path.lower():\n",
        "        print(f\"OCR Text for {image_path}:\\n{ocr_text}\\n\")\n",
        "        # Attempt to parse the table\n",
        "        try:\n",
        "            df = pd.read_csv(io.StringIO(ocr_text))\n",
        "            print(f\"DataFrame for {image_path}:\\n{df}\\n\")\n",
        "        except:\n",
        "            print(f\"Could not parse table from {image_path}\\n\")\n"
      ],
      "metadata": {
        "id": "AOLB4I3wkyuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**7. Display Results**\n",
        "Display Images with Captions and OCR Text"
      ],
      "metadata": {
        "id": "NvLeN-dbjQDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for image_path in image_paths:\n",
        "    print(f\"Image: {image_path}\")\n",
        "    display(IPythonImage(filename=image_path))\n",
        "    print(f\"Caption: {captions.get(image_path, 'No caption available')}\\n\")\n",
        "    print(f\"OCR Text:\\n{ocr_results.get(image_path, 'No OCR text available')}\\n\")\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "id": "atoj7OSolidb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**8. Advanced Interpretation (Optional)**\n",
        "Summarize the Extracted Text"
      ],
      "metadata": {
        "id": "1kk8IrECjaTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load summarization pipeline\n",
        "summarizer = pipeline(\"summarization\")\n",
        "\n",
        "# Due to token limits, you may need to summarize in chunks.\n",
        "chunk_size = 1000  # Adjust based on your needs\n",
        "text_chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "\n",
        "summaries = []\n",
        "for chunk in text_chunks:\n",
        "    summary = summarizer(chunk, max_length=150, min_length=40, do_sample=False)\n",
        "    summaries.append(summary[0]['summary_text'])\n",
        "\n",
        "# Combine summaries\n",
        "full_summary = ' '.join(summaries)\n",
        "print(\"Summary of the Article:\\n\")\n",
        "print(full_summary)\n"
      ],
      "metadata": {
        "id": "jxWe-Ceilk-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Notes and Considerations\n",
        "- Tesseract OCR: Colab comes with Tesseract OCR installed, but if you encounter any issues, ensure it's installed and properly configured.\n",
        "\n",
        "- GPU Support: If you're working with large models or need faster computation, consider enabling GPU acceleration in Colab: Runtime > Change runtime type > Hardware accelerator > GPU.\n",
        "\n",
        "- Model Performance: The pre-trained models may not perfectly interpret complex scientific images or tables. For better results, consider fine-tuning models or using specialized models.\n",
        "\n",
        "- Error Handling: Ensure to handle exceptions, especially when dealing with OCR and parsing, to avoid interruptions in the workflow.\n",
        "\n",
        "- Data Privacy: Be cautious when uploading proprietary or sensitive documents to Colab, as they are processed on external servers."
      ],
      "metadata": {
        "id": "PTgbAOsRj7Rr"
      }
    }
  ]
}